{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "006f2d99"
      },
      "source": [
        "This cell installs the `captum` library, which is essential for implementing interpretability methods like Integrated Gradients and Layer-wise Relevance Propagation (LRP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58791a53"
      },
      "source": [
        "!pip install captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65223633"
      },
      "source": [
        "This cell forcefully reinstalls `numpy` and `scikit-image`. This step is often necessary to resolve potential dependency conflicts that can arise with different library versions, ensuring a stable environment for image processing and numerical operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad1e98dd"
      },
      "source": [
        "!pip install --force-reinstall numpy scikit-image --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a522ab24"
      },
      "source": [
        "This cell imports all the necessary Python libraries for the project. These include `torch` for deep learning, `torchvision` for computer vision utilities, `numpy` for numerical operations, `matplotlib` for plotting, `PIL` for image manipulation, `captum` for model interpretability, `scikit-image` and `sklearn` for image processing and clustering, and `cv2` (OpenCV) for additional image functionalities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec574d43"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from captum.attr import IntegratedGradients\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "from sklearn.cluster import MeanShift, AgglomerativeClustering\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "import cv2\n",
        "from matplotlib import patches\n",
        "from PIL import Image\n",
        "from itertools import combinations\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913127be"
      },
      "source": [
        "!pip install torchxrayvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65a24132"
      },
      "source": [
        "This cell initializes a pre-trained ResNet-18 model from `torchvision`. `pretrained=True` means the model comes with weights trained on the ImageNet dataset. `model.eval()` sets the model to evaluation mode, disabling dropout and batch normalization updates, which is standard practice when performing inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5166fb82"
      },
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402b759c"
      },
      "source": [
        "This cell defines a series of image transformations to be applied to the input image. These transformations include resizing the image to 224x224 pixels, converting it to a PyTorch tensor, and normalizing its pixel values using the mean and standard deviation typical for ImageNet-trained models. This ensures the input format is compatible with the pre-trained ResNet-18 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7ae4b0"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b8c3a0"
      },
      "source": [
        "This cell loads an image from the specified `image_path` (in this case, 'ambulance.png'), converts it to RGB format, and applies the predefined `transformations`. The `unsqueeze(0)` adds a batch dimension, making the tensor ready for input into the neural network. `input_tensor.requires_grad = True` is crucial for interpretability methods that rely on gradients, like LRP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d7d2f49"
      },
      "source": [
        "image_path = \"/content/ambulance.png\"\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(image).unsqueeze(0)\n",
        "input_tensor.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264f888c"
      },
      "source": [
        "This cell performs a forward pass through the ResNet-18 model with the prepared `input_tensor`. It then calculates the softmax probabilities for the output, identifies the predicted class (the class with the highest probability), and prints the prediction details, including the top 5 classes and their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3b14d15"
      },
      "source": [
        "output = model(input_tensor)\n",
        "probabilities = torch.softmax(output, dim=1)\n",
        "prediction = torch.argmax(output).item()\n",
        "predicted_prob = probabilities[0, prediction].item()\n",
        "print(f\"Predicted Class: {prediction} with probability {predicted_prob:.4f}\")\n",
        "\n",
        "topk_probs, topk_indices = torch.topk(probabilities, k=5, dim=1)\n",
        "print(\"Top 5 classes and probabilities:\")\n",
        "for i in range(5):\n",
        "    print(f\"Class {topk_indices[0, i].item()}: {topk_probs[0, i].item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addbd317"
      },
      "source": [
        "This cell defines the `compute_lrp` function, which implements Layer-wise Relevance Propagation (LRP). LRP is an interpretability technique that decomposes the prediction of a neural network layer by layer, attributing relevance scores to input features. The function computes the relevance for a target class by backpropagating a one-hot gradient and taking the absolute sum of the gradient across color channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e407c170"
      },
      "source": [
        "def compute_lrp(model, input_tensor, target_class):\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0, target_class] = 1\n",
        "    output.backward(gradient=one_hot)\n",
        "    relevance = input_tensor.grad.clone().detach()\n",
        "    relevance = relevance.abs().sum(dim=1)[0]\n",
        "    return relevance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb4620a"
      },
      "source": [
        "This cell computes the LRP heatmap for the predicted class using the `compute_lrp` function. The resulting relevance map is then normalized to a range of 0 to 1, making it easier to visualize and compare the importance of different image regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "068b3130"
      },
      "source": [
        "lrp_map = compute_lrp(model, input_tensor, prediction).cpu().numpy()\n",
        "lrp_map = (lrp_map - lrp_map.min()) / (lrp_map.max() - lrp_map.min() + 1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1470cb38"
      },
      "source": [
        "This cell computes the Integrated Gradients (IG) attribution map. Integrated Gradients is another popular interpretability technique that attributes the prediction of a deep learning model to its input features. It does this by integrating gradients along a path from a baseline input (here, a zero tensor) to the actual input. The resulting `ig_map` is then processed to highlight positive contributions and normalized for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fc52228"
      },
      "source": [
        "ig = IntegratedGradients(model)\n",
        "baseline = torch.zeros_like(input_tensor)\n",
        "attributions, _ = ig.attribute(input_tensor, baseline, target=prediction, return_convergence_delta=True)\n",
        "\n",
        "ig_map = attributions.squeeze().detach().numpy()\n",
        "ig_map = np.transpose(ig_map, (1, 2, 0))\n",
        "ig_map = np.mean(ig_map, axis=2)\n",
        "ig_map = np.maximum(ig_map, 0)\n",
        "ig_map /= ig_map.max() + 1e-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e57a47e3"
      },
      "source": [
        "This cell defines the `threshold_map` function, a utility to create a binary mask from a saliency map. It identifies pixels whose relevance scores are above a specified percentile, effectively highlighting the most important regions according to the saliency map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3457b263"
      },
      "source": [
        "def threshold_map(saliency_map, percentile=80):\n",
        "    threshold = np.percentile(saliency_map, percentile)\n",
        "    return (saliency_map >= threshold).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a0973c"
      },
      "source": [
        "This cell applies the `threshold_map` function to both the LRP and IG heatmaps, creating binary masks (`lrp_thresh` and `ig_thresh`). It then computes the `intersection` of these two masks, representing pixels commonly highlighted by both methods. Finally, it visualizes the LRP heatmap, IG heatmap, their intersection, and an overlay of the individual thresholds, providing a comparative view of the model's focus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22f6d766"
      },
      "source": [
        "lrp_thresh = threshold_map(lrp_map, percentile=95)\n",
        "ig_thresh = threshold_map(ig_map, percentile=95)\n",
        "\n",
        "intersection = (lrp_thresh & ig_thresh).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(lrp_map, cmap='gray')\n",
        "plt.title(\"LRP Heatmap\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(ig_map, cmap='gray')\n",
        "plt.title(\"IG Heatmap\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(intersection, cmap='gray')\n",
        "plt.title(\"Common Pixels (IG ∩ LRP)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow((lrp_thresh + ig_thresh), cmap='gray')\n",
        "plt.title(\"Overlay of LRP + IG Thresholds\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a0676d"
      },
      "source": [
        "This cell defines the `cluster_agglomerative` function, which performs Agglomerative Clustering on the binary map of common pixels. This unsupervised learning technique groups spatially adjacent and highly relevant pixels into distinct 'causal concepts' or clusters. If no relevant pixels are found, it returns an empty map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ef6e995"
      },
      "source": [
        "def cluster_agglomerative(binary_map, n_clusters=10, linkage='ward'):\n",
        "    coords = np.argwhere(binary_map == 1)\n",
        "    if coords.shape[0] == 0:\n",
        "        return np.zeros_like(binary_map, dtype=int)\n",
        "\n",
        "    agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
        "    agg.fit(coords)\n",
        "\n",
        "    labels = agg.labels_\n",
        "    labeled_map = np.full(binary_map.shape, -1, dtype=int)\n",
        "    for i, coord in enumerate(coords):\n",
        "        labeled_map[coord[0], coord[1]] = labels[i]\n",
        "\n",
        "    return labeled_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96a1715"
      },
      "source": [
        "This cell applies the `cluster_agglomerative` function to the `intersection` map (common pixels from LRP and IG) to identify 10 distinct clusters using the 'ward' linkage method. It then masks out the background and visualizes these clusters using a distinct color map, showing the spatial grouping of the model's most relevant input regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf1317a9"
      },
      "source": [
        "# Example usage\n",
        "agg_labels = cluster_agglomerative(intersection, n_clusters=10, linkage='ward')\n",
        "\n",
        "# Mask out background (-1) and set its color\n",
        "masked_labels = np.ma.masked_where(agg_labels == -1, agg_labels)\n",
        "\n",
        "# Custom colormap for 10 clusters\n",
        "cmap = plt.get_cmap('tab10', 10)\n",
        "cmap.set_bad(color='white')  # Change this to any background color you want\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "im = ax.imshow(masked_labels, cmap=cmap)\n",
        "ax.set_title(\"Agglomerative Clustering (10 clusters)\")\n",
        "ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f874012d"
      },
      "source": [
        "This cell defines the `unnormalize` function. This utility is crucial for visualizing images that have been normalized for neural network input. It reverses the normalization process using the original mean and standard deviation, converting the tensor back to a human-readable pixel value range (e.g., 0-1 or 0-255)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ca97ee3"
      },
      "source": [
        "def unnormalize(img_tensor, mean, std):\n",
        "    img_tensor = img_tensor.clone()\n",
        "    for i in range(img_tensor.shape[0]):\n",
        "        img_tensor[i] = img_tensor[i] * std[i] + mean[i]\n",
        "    return img_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3d55b3"
      },
      "source": [
        "This cell uses the `unnormalize` function to convert the model's input tensor back into a human-readable NumPy array. It applies the inverse of the normalization process and clips the pixel values to the valid range (0-1), preparing the image for display without the normalization artifacts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63dcb459"
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "transformed_img = unnormalize(input_tensor[0], mean, std)\n",
        "transformed_img_np = transformed_img.detach().cpu().numpy().transpose(1, 2, 0)\n",
        "transformed_img_np = np.clip(transformed_img_np, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c08edb4"
      },
      "source": [
        "This cell defines the `draw_cluster_boundaries` function. This function takes an image and a labeled map of clusters, then draws bounding boxes around each identified cluster. It also labels each bounding box with its cluster ID, providing a clear visual representation of the segmented causal concepts on the original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35f20934"
      },
      "source": [
        "def draw_cluster_boundaries(image_np, labeled_map, title=\"Cluster Boundaries\", skip_noise=True):\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    clusters = np.unique(labeled_map)\n",
        "    for cl in clusters:\n",
        "        if skip_noise and cl == -1:\n",
        "            continue\n",
        "        coords = np.argwhere(labeled_map == cl)\n",
        "        if coords.size == 0:\n",
        "            continue\n",
        "        min_row, min_col = coords.min(axis=0)\n",
        "        max_row, max_col = coords.max(axis=0)\n",
        "        width = max_col - min_col\n",
        "        height = max_row - min_row\n",
        "\n",
        "        rect = patches.Rectangle((min_col, min_row), width, height, linewidth=2,\n",
        "                                 edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(min_col, min_row, f\"{cl}\", color='yellow', fontsize=12, weight='bold')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26da69eb"
      },
      "source": [
        "This cell calls the `draw_cluster_boundaries` function to visualize the identified clusters (`agg_labels`) on the unnormalized and transformed image (`transformed_img_np`). This creates an overlay of bounding boxes, clearly showing where each causal concept is located on the original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27d953a3"
      },
      "source": [
        "draw_cluster_boundaries(transformed_img_np, agg_labels, title=\"Mean-Shift Cluster Boundaries on Transformed Image\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4162d7c"
      },
      "source": [
        "This cell defines the `zero_bbox` function, a utility used for ablation studies. It takes an image and a bounding box, then sets all pixel values within that bounding box to zero (effectively blacking out that region). This allows us to assess the impact of specific image regions on the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58b069fc"
      },
      "source": [
        "def zero_bbox(image_np, bbox):\n",
        "    modified_image = image_np.copy()\n",
        "    modified_image[bbox[0]:bbox[2], bbox[1]:bbox[3], :] = 0\n",
        "    return modified_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35c1a221"
      },
      "source": [
        "This cell simply prints the baseline prediction: the predicted class and its probability for the original, unmodified image. This serves as a reference point for comparing how predictions change when different causal concepts are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f751ae6c"
      },
      "source": [
        "print(f\"Baseline Prediction: Class {prediction} with probability {predicted_prob:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48068e1"
      },
      "source": [
        "This cell performs a 'single-concept ablation' study. It iterates through each identified cluster (causal concept), zeros out its bounding box on a copy of the original image, and then re-evaluates the model's prediction. For each ablated image, it displays the modified image, its new prediction, and then summarizes the results by comparing them to the baseline prediction. This helps understand the individual contribution of each concept to the model's decision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cccc8be"
      },
      "source": [
        "unique_clusters = np.unique(agg_labels)\n",
        "results = []\n",
        "\n",
        "original_np = (transformed_img_np * 255).astype(np.uint8)\n",
        "\n",
        "for cl in unique_clusters:\n",
        "    if cl == -1:\n",
        "        continue\n",
        "\n",
        "    coords = np.argwhere(agg_labels == cl)\n",
        "    min_row, min_col = coords.min(axis=0)\n",
        "    max_row, max_col = coords.max(axis=0)\n",
        "    bbox = (min_row, min_col, max_row + 1, max_col + 1)\n",
        "\n",
        "    modified_np = zero_bbox(original_np, bbox)\n",
        "    modified_img = Image.fromarray(modified_np)\n",
        "\n",
        "    input_tensor_modified = transform(modified_img).unsqueeze(0)\n",
        "    input_tensor_modified.requires_grad = True\n",
        "\n",
        "    output_modified = model(input_tensor_modified)\n",
        "    probabilities_modified = torch.softmax(output_modified, dim=1)\n",
        "    prediction_modified = torch.argmax(output_modified).item()\n",
        "    predicted_prob_modified = probabilities_modified[0, prediction_modified].item()\n",
        "\n",
        "    results.append((cl, prediction_modified, predicted_prob_modified))\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(modified_np)\n",
        "    plt.title(f\"Causal Concept {cl} Zeroed Out:\\nPredicted Class {prediction_modified} (Prob: {predicted_prob_modified:.4f})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nComparison with Baseline:\")\n",
        "for cl, pred, prob in results:\n",
        "    print(f\"Causal Concept {cl}: Modified Prediction - Class {pred} (Probability: {prob:.4f}) vs Baseline Class {prediction} (Probability: {predicted_prob:.4f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3565b6"
      },
      "source": [
        "This cell defines the `compute_pcs_and_bf` function, which calculates two metrics to quantify the impact of removing a causal concept: Probability Change Score (PCS) and Bayes Factor (BF). PCS measures the relative drop in probability for the baseline class, while BF quantifies how much more likely the original prediction is compared to the modified prediction, given the observed probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6a11a3"
      },
      "source": [
        "def compute_pcs_and_bf(p_orig, p_mod, epsilon=1e-6):\n",
        "    p_orig = np.clip(p_orig, epsilon, 1 - epsilon)\n",
        "    p_mod = np.clip(p_mod, epsilon, 1 - epsilon)\n",
        "\n",
        "    pcs = (p_orig - p_mod) / p_orig\n",
        "\n",
        "    bf = (p_orig / (1 - p_orig)) / (p_mod / (1 - p_mod))\n",
        "\n",
        "    return pcs, bf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8ad7df"
      },
      "source": [
        "This cell defines the `plot_concept_summary` function. This function visualizes the impact of removing each causal concept (cluster) on the model's prediction of the baseline class. It calculates the change in probability for the baseline class when each cluster's bounding box is zeroed out and then plots these changes as a bar chart, indicating positive or negative contributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13d30687"
      },
      "source": [
        "def unnormalize_and_to_np(img_tensor, mean, std):\n",
        "    x = img_tensor.clone()\n",
        "    for i in range(x.shape[0]):\n",
        "        x[i] = x[i] * std[i] + mean[i]\n",
        "    x_np = x.detach().cpu().numpy().transpose(1, 2, 0)\n",
        "    x_np = np.clip(x_np, 0, 1)\n",
        "    return (x_np * 255).astype(np.uint8)\n",
        "\n",
        "def plot_concept_summary(\n",
        "    input_tensor, agg_labels, model, transform,\n",
        "    prediction, predicted_prob\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots the per-cluster probability drop after zeroing out bounding boxes.\n",
        "    Uses the same cluster-label map as single_concept_ablation().\n",
        "    \"\"\"\n",
        "\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std  = [0.229, 0.224, 0.225]\n",
        "    original_np = unnormalize_and_to_np(input_tensor[0], mean, std)\n",
        "\n",
        "    baseline_class = prediction\n",
        "    baseline_prob = predicted_prob\n",
        "\n",
        "    clusters = [cl for cl in np.unique(agg_labels) if cl != -1]\n",
        "    effects = []\n",
        "    concept_labels = []\n",
        "\n",
        "    def get_bbox_for_cluster(cluster_label, label_map):\n",
        "        coords = np.argwhere(label_map == cluster_label)\n",
        "        min_row, min_col = coords.min(axis=0)\n",
        "        max_row, max_col = coords.max(axis=0)\n",
        "        return (min_row, min_col, max_row + 1, max_col + 1)\n",
        "\n",
        "    for cl in clusters:\n",
        "        bbox = get_bbox_for_cluster(cl, agg_labels)\n",
        "        modified_np = zero_bbox(original_np, bbox)\n",
        "        modified_img = Image.fromarray(modified_np)\n",
        "        input_mod = transform(modified_img).unsqueeze(0)\n",
        "        input_mod.requires_grad = True\n",
        "\n",
        "        output_mod = model(input_mod)\n",
        "        probs_mod = torch.softmax(output_mod, dim=1)\n",
        "        mod_prob = probs_mod[0, baseline_class].item()\n",
        "\n",
        "        diff = mod_prob - baseline_prob\n",
        "        effects.append(diff)\n",
        "        concept_labels.append(cl)\n",
        "\n",
        "    # Plot the summary\n",
        "    colors = ['green' if diff < 0 else 'red' for diff in effects]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar([str(c) for c in concept_labels], effects, color=colors)\n",
        "    plt.xlabel(\"Causal Concept\")\n",
        "    plt.ylabel(\"Change in Baseline Class Probability\\n(Modified - Baseline)\")\n",
        "    plt.title(\"Effect of Removing Each Causal Concept on Model Prediction\")\n",
        "    plt.axhline(0, color='black', linewidth=0.8)\n",
        "\n",
        "    for bar, diff in zip(bars, effects):\n",
        "        height = bar.get_height()\n",
        "        plt.annotate(f\"{diff:.3f}\",\n",
        "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                     xytext=(0, 3),\n",
        "                     textcoords=\"offset points\",\n",
        "                     ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Summary of Causal Concept Effects (per concept):\")\n",
        "    for cl, diff in zip(concept_labels, effects):\n",
        "        effect_type = \"Positive Contribution\" if diff < 0 else \"Negative Effect\"\n",
        "        print(f\"Concept {cl}: Change = {diff:.4f} → {effect_type}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f263d0d2"
      },
      "source": [
        "This cell calls the `plot_concept_summary` function, which visualizes the impact of removing each causal concept (cluster) on the model's prediction for the baseline class. The plot shows the change in probability, helping to identify which concepts positively or negatively influence the model's decision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0470a99b"
      },
      "source": [
        "plot_concept_summary(\n",
        "    input_tensor=input_tensor,\n",
        "    agg_labels=agg_labels,\n",
        "    model=model,\n",
        "    transform=transform,\n",
        "    prediction=prediction,\n",
        "    predicted_prob=predicted_prob\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b76a65"
      },
      "source": [
        "This cell defines the `plot_pcs_visualization` function. This function extends the causal concept analysis by visualizing the Probability Change Score (PCS) and Bayes Factor (BF) for each cluster directly on the image. It draws bounding boxes around each concept, color-coded by their PCS, providing an intuitive understanding of which regions have the most significant impact on the model's output when removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14db8e83"
      },
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.colors import Normalize\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def plot_pcs_visualization(\n",
        "    input_tensor,\n",
        "    agg_labels,\n",
        "    model,\n",
        "    transform,\n",
        "    prediction,\n",
        "    predicted_prob,\n",
        "    compute_pcs_and_bf  # a function that returns (pcs, bf)\n",
        "):\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "    original_np = unnormalize_and_to_np(input_tensor[0], mean, std)\n",
        "\n",
        "    def get_bbox(cluster_label):\n",
        "        coords = np.argwhere(agg_labels == cluster_label)\n",
        "        min_row, min_col = coords.min(axis=0)\n",
        "        max_row, max_col = coords.max(axis=0)\n",
        "        return (min_row, min_col, max_row + 1, max_col + 1)\n",
        "\n",
        "    detailed_results = []\n",
        "\n",
        "    clusters = [cl for cl in np.unique(agg_labels) if cl != -1]\n",
        "\n",
        "    for cl in clusters:\n",
        "        bbox = get_bbox(cl)\n",
        "\n",
        "        modified_np = zero_bbox(original_np, bbox)\n",
        "        modified_img = Image.fromarray(modified_np)\n",
        "        input_mod = transform(modified_img).unsqueeze(0)\n",
        "        input_mod.requires_grad = True\n",
        "\n",
        "        output = model(input_mod)\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        mod_prob = probs[0, prediction].item()\n",
        "        mod_pred = torch.argmax(probs).item()\n",
        "\n",
        "        pcs, bf = compute_pcs_and_bf(predicted_prob, mod_prob)\n",
        "\n",
        "        detailed_results.append({\n",
        "            'cluster': cl,\n",
        "            'modified_pred': mod_pred,\n",
        "            'modified_prob': mod_prob,\n",
        "            'pcs': pcs,\n",
        "            'bf': bf,\n",
        "            'bbox': bbox\n",
        "        })\n",
        "\n",
        "    # Sort by PCS descending\n",
        "    detailed_results.sort(key=lambda x: x['pcs'], reverse=True)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.imshow(original_np)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    cmap = cm.get_cmap(\"Reds\")\n",
        "    norm = Normalize(vmin=0, vmax=max(r['pcs'] for r in detailed_results))\n",
        "\n",
        "    for res in detailed_results:\n",
        "        cl = res['cluster']\n",
        "        pcs = res['pcs']\n",
        "        bf = res['bf']\n",
        "        bbox = res['bbox']\n",
        "        color = cmap(norm(pcs))\n",
        "\n",
        "        rect = Rectangle(\n",
        "            (bbox[1], bbox[0]),\n",
        "            bbox[3] - bbox[1],\n",
        "            bbox[2] - bbox[0],\n",
        "            linewidth=2,\n",
        "            edgecolor=color,\n",
        "            facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(\n",
        "            bbox[1], bbox[0] - 5,\n",
        "            f\"#{cl} | PCS: {pcs:.2f} | BF: {bf:.1f}\",\n",
        "            color='black', fontsize=9,\n",
        "            bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.3')\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06a8f04"
      },
      "source": [
        "This cell calls the `plot_pcs_visualization` function, which visualizes the Probability Change Score (PCS) and Bayes Factor (BF) for each causal concept (cluster). The plot displays bounding boxes around each concept on the original image, with colors indicating their PCS, offering a detailed visual analysis of their individual causal contributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2efe5986"
      },
      "source": [
        "plot_pcs_visualization(\n",
        "    input_tensor=input_tensor,\n",
        "    agg_labels=agg_labels,\n",
        "    model=model,\n",
        "    transform=transform,\n",
        "    prediction=prediction,\n",
        "    predicted_prob=predicted_prob,\n",
        "    compute_pcs_and_bf=compute_pcs_and_bf\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECLARATION:\n",
        "\n",
        "Generative AI tools were used to write some parts of the code and explainations of the code in this notebook."
      ],
      "metadata": {
        "id": "uL0ctcLka-rQ"
      }
    }
  ]
}